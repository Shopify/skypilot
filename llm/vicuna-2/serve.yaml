resources:
  accelerators: A100:1
  disk_size: 1024
  disk_tier: high
  memory: 32+

file_mounts:
  /model_ckpt:
    source: $MODEL_CKPT
    mode: COPY

setup: |
  conda activate chatbot
  if [ $? -ne 0 ]; then
    conda create -n chatbot python=3.10 -y
    conda activate chatbot
  fi

  # Install dependencies
  pip install git+https://github.com/lm-sys/FastChat.git


run: |
  conda activate chatbot
  
  echo 'Starting controller...'
  python -u -m fastchat.serve.controller > ~/controller.log 2>&1 &
  sleep 10
  echo 'Starting model worker...'
  python -u -m fastchat.serve.model_worker \
            --model-path /model_ckpt 2>&1 \
            | tee model_worker.log &

  echo 'Waiting for model worker to start...'
  while ! `cat model_worker.log | grep -q 'Uvicorn running on'`; do sleep 1; done

  echo 'Starting gradio server...'
  python -u -m fastchat.serve.gradio_web_server --share | tee ~/gradio.log

envs:
  MODEL_SIZE: 7
  MODEL_CKPT: <bucket-path-to-your-model-ckpt>
